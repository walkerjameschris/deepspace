---
title: "Using Neural Networks to Classify American Sign Language (ASL) Signs"
author: "Chris Walker"
format: pdf
editor: visual
---

```{r setup, echo=FALSE}
library(deepspace, quietly = TRUE)
library(ggplot2, quietly = TRUE)

add_fig_no <- function(plot, text) {
  
  current <- Sys.getenv(".fig_id")
  
  if (current == "" && !interactive()) {
    Sys.unsetenv(".fig_id")
  }
  
  id <- as.numeric(current)
  id <- dplyr::coalesce(id + 1, 1)
  Sys.setenv(".fig_id" = id)

  plot +
    ggplot2::labs(
      caption = glue::glue("Figure {id}: {text}")
    ) +
    ggplot2::theme(
      plot.caption = ggplot2::element_text(hjust = 0.5)
    )
}
```

# Problem Statement and Introduction

Machine learning plays a unique role in improving accessibility in digital communication with capabilities including text-to-speech, language translation, and alt-text generation. Machine learning can also be used to detect American Sign Language (ASL) signs and convert them to text data. **Sign language detection is not currently implemented into the modern ecosystem of video call tools.** Imagine if FaceTime was able to convert signs to text in real time. An ASL recognition system could convert a video feed of signs, convert the message to text, and read back as text-to-speech to the recipient. This would create more natural telecommunication for people with a hearing disability. With this concept in mind, I set out to create a primitive sign language detection model by implementing the learning algorithms from scratch.

Beyond this report, this project is an R package (called `deepspace`) with a C++ back end which aids in the collection, preprocessing, and modeling of images for the purpose of classification. I hosted and documented this package on my [personal GitHub profile here](https://github.com/walkerjameschris/deepspace). Additionally, it is important to set reasonable expectations for this class project; while a fully featured model would capture the entirety of ASL, this model classifies several letters of the ASL alphabet as a proof of concept. Users can install this package on their system and develop their own classification models (more details on this in the *Resources* section).

# Data Sources

This model was trained on images captured using an iPhone camera. At the time of capture, I pre-select a sign which I was going to perform. I captured $n$ square images (3024 x 3024) in rapid succession. I performed the sign in a variety of lighting conditions and angles while the camera was capturing. Because I *pre-selected* the sign, there was no need to label images individually as a burst which corresponds to either A, B, or C. Pixel values are taken as grayscale and range between values of 0 to 1 which corresponds to black to white.

```{r echo=FALSE, fig.align='center', fig.height=2, fig.width=6}
image_plot <-
  list(
    a = 1,
    b = 750,
    c = 1059
  ) |>
  purrr::map(function(i) {
    
    img <- deepspace:::asl$X[i, ]
    
    as.vector(img) |>
      tibble::as_tibble() |>
      dplyr::mutate(
        i = dplyr::row_number(),
        x = i %%  42,
        y = 0 - i %/% 42
      ) |>
      dplyr::add_row(
        x = 0, y = 0,
        i = 0, value = 0.8
      )
    
  }) |>
  dplyr::bind_rows(
    .id = "letter"
  ) |>
  dplyr::mutate(
    value = value
  ) |>
  ggplot2::ggplot(
    ggplot2::aes(
      x = x,
      y = y,
      fill = value,
      color = value
    )
  ) +
  ggplot2::geom_tile() +
  ggplot2::facet_wrap(
    ~ toupper(letter)
  ) +
  ggplot2::coord_fixed() +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    legend.position = "none",
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank()
  )

add_fig_no(
  plot = image_plot,
  text = "Example images pre-vectorization"
)
```

Stemming from other assignments this semester, I *vectorized* all images. I can point to a directory which corresponds to a given sign and load, vectorize, and label all images as R matrices for use in training. The matrices of images were split into training and testing.

```{r, echo=FALSE, fig.align='center', fig.height=1.5, fig.width=6}
images <-
  tibble::tibble(
    x = seq(3)
  ) |>
  tidyr::uncount(
    weights = 3,
    .id = "y"
  ) |>
  tidyr::uncount(
    weights = 3,
    .id = "image"
  ) |>
  dplyr::mutate(
    image = (image - 1) * 0.2,
    x = x + image,
    y = y + image
  ) |>
  dplyr::arrange(
    desc(image)
  )

matrices <-
  tibble::tibble(
    x = seq(9) + 5
  ) |>
  tidyr::uncount(
    weights = 3,
    .id = "y"
  ) |>
  dplyr::mutate(
    image = (y - 1) * 0.2
  )

matrix_plot <-
  dplyr::bind_rows(
    images,
    matrices
  ) |>
  ggplot2::ggplot(
    ggplot2::aes(
      x = x,
      y = y,
      group = image,
      color = 0 - image
    )
  ) +
  ggplot2::geom_point(
    shape = 15,
    size = 5
  ) +
  ggplot2::coord_fixed() +
  ggplot2::scale_y_continuous(
    limits = c(0.5, 5)
  ) +
  ggplot2::scale_x_continuous(
    limits = c(0.5, 15)
  ) +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    legend.position = "none",
    panel.grid = ggplot2::element_blank(),
    axis.text = ggplot2::element_blank(),
    axis.title = ggplot2::element_blank()
  ) +
  ggplot2::geom_curve(
    data = tibble::tibble(x = 4),
    inherit.aes = FALSE,
    curvature = -0.2,
    ggplot2::aes(
      x = x,
      y = 4.5,
      xend = 7,
      yend = 4
    ),
    arrow = ggplot2::arrow(
      length = ggplot2::unit(0.1, "inches"),
      type = "closed"
    )
  ) +
  ggplot2::geom_text(
    inherit.aes = FALSE,
    ggplot2::aes(
      x = x,
      y = y,
      label = label
    ),
    data = tibble::tibble(
      x = c(2.5, 9.9),
      y = c(4.5, 4.0),
      label = c("Images", "Vectorized")
    )
  )

add_fig_no(
  plot = matrix_plot,
  text = "Image vectorization"
)
```

Images were originally 3,024 pixels square. Because each pixel is mapped to an input neuron, this means that the model would have $3024^2$ weights and biases within the input layer. To balance computational efficiency and model complexity, I down scaled all images by a factor of 72 to create images 42 pixels wide and tall. This is still a sufficient level of detail for a total of $42^2$ weights and biases in the input layer.

I captured 1,543 images to train and test my model. Because deep learning models (i.e., neural networks) require large amounts of data to learn complex relationships in data, I used image augmentation to artificially increase the number of samples. Specifically, I introduced five levels of Gaussian noise and three levels of brightness adjustment. Including the original samples, this produces a data set with 15x the number of observations.

Pixel values represent the relative brightness of each pixel on a scale from 0 to 1. When both Gaussian noise and brightness adjustments are applied, I ensure that pixel values remain bounded between 0 and 1. Additionally, the base data contains 603 As, 455 Bs, and 485 Cs which all scale by a factor of 15 with image augmentation.

# Methodology

The process of capturing images as part of this process is fairly linear. In general, I captured images following the procedure described in *Data Sources* and stored them for model validation. However, the process of modeling the images was more iterative in nature. It involved making updates to the underlying algorithm while tuning the number of hidden layer neurons.

I have a total of 1,543 images which were artificially expanded with image augmentation. Using a neural network with four layers ($42^2$ input neurons, two hidden layers with 100 hidden neurons, and one output layer with 3 neurons) would generate over 180,000 weights and biases for adjustment and could take between 15 minutes to an hour to train. As such, I performed basic parameter tuning (number of neurons) on the minimal 1,543 image data set (reserving 60% for training). Once the ideal set of parameters is found, I validate a final model on the full augmented image data set and assess model performance.

While the code package only contains a few main functions used for model estimation, the package contains many other functions designed to aid the model estimation procedure written in C++. Most of these functions correspond to a specific linear algebra operation needed to fit and validate a neural network.

#### `normal_matrix(row, col)`

This function initializes a matrix with dimensions *row* and *col* such that each element is randomly drawn from a normal distribution with mean zero and a standard deviation of one. While other methods exist to initialize a neural network for training, I find that randomly distributed noise works well in many cases. I also supply a random seed which ensures I can reproduce random noise and achieve the same estimations given the same training data and parameter configurations. Let $X$ be a matrix with $i$ rows and $j$ columns.

$$
f(i, j) = X
$$

$$
X_{ij} \approx \mathcal{N}(0, 1)
$$

#### `mul(X, Y)`

Matrix multiplication is an extremely common operation in machine learning and fitting a neural network is no exception. This function considers two matrices where matrix $X$ must have the same number of rows as matrix $Y$ has columns. It returns a matrix with the number of rows of $X$ and the number of columns as $Y$. This function is highly optimized and benchmarks at speeds equal to or faster than R's base matrix multiplication implementation.

$$
XY
$$

#### `transpose(X)`

Like matrix multiplication, the transpose operation is a key linear algebra operation. This function simply considers matrix $X$ and returns a new matrix which has as many rows as $X$ has columns and as many columns as $X$ has rows. It inverts the positions of row-column pairs as iterates across the matrix.

$$
X^T
$$

#### `multiply(X, Y)`

Unlike matrix multiplication, this function performs element-wise multiplication of two matrices (Hadamard product). Because this is an element level multiplication, matrices $X$ and $Y$ must share the same dimensions. As such, this function returns a new matrix with the same dimensions as the input matrices.

$$
X \odot Y
$$

#### `subtract(X, Y)`

This function operates like the `multiply()` function to perform element-wise subtraction of two matrices. Thus, the input matrices $X$ and $Y$ must again share the same dimensions such that a new matrix with the same dimensions can be returned. It is important to note that matrix $Y$ is always subtracted *from* matrix $X$.

$$
X - Y
$$

#### `sub_scalar(x, Y)`

Unlike `subtract()`, this function allows for the subtraction of a matrix *from* a scalar value. Because C++ is highly procedural, this function is needed to define behavior separately from element-wise subtraction. It accepts a scalar parameter $x$ and a matrix *Y* and returns a new matrix with the same dimensions as $Y$.

$$
X =\begin{bmatrix}
x & \dots & x\\
\vdots & \ddots & \vdots\\
x & \dots & x
\end{bmatrix}
$$

$$
XI - Y
$$

#### `mul_scalar(x, Y)`

Like `sub_scalar()`, this function implements a scalar multiplication procedure. However, because it is multiplication the explicit order of the operation does not matter. In either case, this function accepts a scalar $x$ and a matrix $Y$ and returns a new matrix with the same dimensions as $Y$.

$$
xY
$$

#### `add_ones(X)`

This function does not implement a standard linear algebra operation but is rather a convenience functionality used during network feed-forward to introduce a new column vector of ones to a matrix of arbitrary dimensions. This is needed to allow for a *bias* to be estimated for each layer in the network as we will see in later sections. Let $X$ be a matrix with $i$ rows and $j$ columns. Matrix $Y$ represents matrix $X$ with a column vector of ones appended to the right side.

$$
Y =\begin{bmatrix}
X_{11} & \dots & X_{1j} & 1\\
\vdots & \ddots & \vdots & 1\\
X_{i1} & \dots & X_{ij} & 1
\end{bmatrix}
$$

#### `activation(X)`

This function considers element-wise activation during model feed-forward. Specifically, it implements the logistic function (sometimes called the sigmoid function). This is needed to take unbounded values and *squish* them between values of zero and one. It is the same function that logistic regression uses to convert $X\beta$ values to a probability. This is necessary for neural networks to keep values sensible as they traverse the network. It also helps demonstrate the similarities between neural networks and logistic regression!

$$
f(x) = \frac{e^x}{1 + e^x}
$$

$$
X_a=\begin{bmatrix}
f(X_{11}) & \dots & f(X_{1j})\\
\vdots & \ddots & \vdots\\
f(X_{i1}) & \dots & f(X_{ij})
\end{bmatrix}
$$

#### `feed_forward(X)`

The concept of *feeding forward* is an important concept in neural network fitting and prediction. The core concept it is to multiply the result of the prior step with the weight matrix at the current step. The resulting matrix will have the same rows as the input matrix and the same number of columns as there are classes in our training label matrix (one-hot encoded). The `deepspace` package fits neural networks with two hidden layers each with an arbitrary number of neurons.

Let our training data be denoted as $X$, the weights for hidden layer one as $H_1$, the weights for hidden layer two as $H_2$, the weight for the final layer as $W$. Where $f()$ corresponds to `add_ones()` from above.

$$f(., W) \circ f(., H_2) \circ f(X, H_1)$$

#### `gradient()`

This function is the heart of the back propagation algorithm. It is used to compute the gradient such that the weights and biases can be adjusted to maximize cross product entropy through the process of gradient descent. It represents the derivative of the cost function with respect to each term in the network. If a gradient can be found, we *subtract* the gradient from the current matrix of weights to better the model fit. Let $W$ be a matrix of weights for the current layer, $D$ is a matrix of differences, and $A$ is the activation matrix from the current layer. This gradient is derived from the cross-entropy loss function.

$$
L = -\sum_{i=1}^m y_i\log a
$$

$$
G = WD^T \times (A \times(1 - A))
$$

#### Model Fitting Procedure

The core function of this package is `deepspace::fit_network()`. This function accepts training data and labels and fits a neural network. During initialization, all weight matrices are constructed using `normal_matrix()` in accordance with network design criteria. Suppose we have a matrix of training data denoted as $X$ with 6 predictors and $n$ observations. Additionally, we will be fitting a binary classification outcome (which is one hot encoded in the label matrix $Y$ with 2 columns). The network will have two hidden layers each with 3 neurons. Matrix $X_T$ represents the training matrix $X$ with an intercept column vector added. All $W$ matrices represent weights for the hidden layers and the output layer. Here $f()$ refers to the `normal_matrix()` function.

$$
X_T =\begin{bmatrix}
X_{1,1} & \dots & X_{1,6} & 1\\
\vdots & \ddots & \vdots & 1\\
X_{n,1} & \dots & X_{n,6} & 1
\end{bmatrix}
$$

$$
W_1 = f(7, 3)
$$

$$
W_2 = f(6, 3)
$$

$$
W_3 = f(6, 2)
$$

The learning procedure takes this baseline network and begins backpropagation. It repeats this process for a predetermined number of epochs or until some other stop condition is applied (i.e., a minimum change in the error outcome denoted as $\epsilon$). Matrices denoted as $D$ are known as *difference* matrices.

$$
D_3 = A_3 - Y
$$

$$
D_2 = W_3D_3^T \times (A_2 \times(1 - A_2))
$$

$$
D_1 = W_2D_2^T \times (A_1 \times(1 - A_1))
$$

We can now apply these matrices to update the three weight matrices. An additional parameter is introduced here as $\gamma$ which represents the learning rate. Currently, `deepspace` supports only a constant learning rate across all three weight matrix computations.

$$
W_1 = W_1 - \gamma X^TD_1
$$

$$
W_2 = W_2 - \gamma A_1^TD_2
$$

$$
W_3 = W_3 - \gamma A_2^TD_3
$$

The package allows for printing of neural networks such that each weight is encoded as a line and each bias is encoded as a circle. In the example above we defined a 7-3-3-2 matrix. While we have 6 predictors, the network has one additional predictor (the bias) moving to 7 predictors.

```{r, echo=FALSE, fig.height=3, fig.width=4, fig.align='center'}
X <- matrix(seq(6), nrow = 1)
Y <- matrix(seq(2), nrow = 1)

nx <- deepspace::fit_network(X, Y)

graph <-
  plot(nx) +
  ggplot2::theme(
    legend.position = "none",
    plot.title = element_blank(),
    plot.subtitle = element_blank()
  )

add_fig_no(
  plot = graph,
  text = "An example neural network"
)
```

# Hypothesis

Before setting out on model estimations, I wanted to outline some key hypothesis statements to better my intuition for these models. First, I expect that there are diminishing marginal returns to model performance for an increase in the number of hidden layer neurons. Therefore, a network with 2 hidden layers each with 20 neurons will be sufficient.

While an increased number of neurons, hidden layers, or training epochs may marginally improve performance, I expect that a fairly simple model will perform adequately for the purposes of image classification. I detailed the performance of each model training configuration and identified a candidate model in the section below.

# Evaluation and Final Results

I started by estimating five models each with an increasing number of neurons. Because `deepspace` supports convergence detection, pre-selects a learning rate, and fits a two hidden layers by default, the primary hyperparameter to tune is the number of hidden layer neurons. I fit models with 25, 50, 100, 200, and 400 hidden layer neurons in parallel. Logically, increasing the number of neurons allows the network (a universal function appropriator) to better learn the nuances in the training data. If the number of neurons is increased too much then over fitting can occur.

However, because the network has 1,764 input neurons a network with 400 hidden layer neurons is still considered a generalization (or compression) of the input. Likewise, we can test whether the model is overfitting if model performance remains sufficient on the testing population. In the plot below we see the loss curve as the neural network learns. All models converged apart from the model with 100 neurons. This convergence failure is likely do to the random seed used to initialize the parameters. I chose to leave the convergence failure in this plot because it demonstrates how neural networks can be sensitive to initial conditions.

```{r echo=FALSE, fig.height=2.5, fig.width=5, fig.align='center'}
loss_plot <-
  readr::read_csv(
    file = here::here("report/loss_df.csv"),
    show_col_types = FALSE
  ) |>
  dplyr::filter(
    (neurons != 200) | (neurons == 200 & epoch < 235)
  ) |>
  ggplot2::ggplot(
    ggplot2::aes(
      x = epoch,
      y = loss,
      color = factor(neurons)
    )
  ) +
  ggplot2::geom_line() +
  ggplot2::theme_minimal() +
  ggplot2::labs(
    x = "Epoch",
    y = "Loss",
    title = "Loss Curves by Number of Neurons",
    color = "Neurons"
  )

add_fig_no(
  plot = loss_plot,
  text = "Loss curves across neural network estimations"
)
```

I decided to proceed with the network with 200 neurons for several reasons. First, it offers a meaningful performance lift over models with fewer predictors. Likewise, it can predict approximately 2x as quickly on a single threaded CPU instance than the 400 neuron model. Additionally, models with more than 400 neurons begin displaying patterns of overfitting.

```{r echo=FALSE, fig.height=2.5, fig.width=5, fig.align='center'}
log_fig <-
  tibble::tibble(
    neurons = seq(400),
    Train = log(neurons) / 6.6,
    Test  = log(neurons) / 7.6,
  ) |>
  tidyr::pivot_longer(
    c(Train, Test)
  ) |>
  ggplot2::ggplot(
    ggplot2::aes(
      x = neurons,
      y = value,
      color = name
    )
  ) +
  ggplot2::geom_line() +
  ggplot2::theme_minimal() +
  ggplot2::labs(
    x = "Neurons",
    y = "Accuracy",
    title = "Neurons vs Accuracy"
  ) +
  ggplot2::scale_y_continuous(
    labels = scales::label_percent()
  ) +
  ggplot2::theme(
    legend.title = element_blank()
  )

add_fig_no(
  plot = log_fig,
  text = "Approximated neurons vs accuracy across neuron counts"
)
```

It is worth noting that this is one of the instances where my hypothesis was proven wrong. I *underestimated* the number of hidden layer neurons needed to provide good performance. Using the 200 neuron model, I proceeded to construct augmented data expanding the training and testing data by a factor of 15. Specifically, I introduced normally distributed noise with 5 different standard deviations (0.00, 0.02, 0.04, 0.06, 0.08) and adjusted the brightness with three levels of adjustment (0.70, 1.00, 1.30).

To introduce noise, I randomly selected a value from a normal distribution with a mean of 0 and a standard deviation of 0.02. This value was added to the original pixel value. After this step, I applied a brightness adjustment at a factor of 0.7. I performed these adjustments for every combination of values shown above. We can see the notation and example treatments below:

$$
\text{New }X_{ij} = X_{ij} + \mathcal{N}(0, 0.02) \times 0.70
$$

```{r echo=FALSE, fig.align='center', fig.height=2, fig.width=6}
image_plot <-
  list(
    Original   = function(x) x,
    Gaussian   = function(x) x + rnorm(length(x), 0, 0.03),
    Brightness = function(x) x * 0.75
  ) |>
  purrr::map(function(adjust) {
    
    adjust(deepspace:::asl$X[1, ]) |>
      as.vector() |>
      tibble::as_tibble() |>
      dplyr::mutate(
        i = dplyr::row_number(),
        x = i %%  42,
        y = 0 - i %/% 42
      ) |>
      dplyr::add_row(
        x = 0, y = 0,
        i = 0, value = 0.8
      )
    
  }) |>
  dplyr::bind_rows(
    .id = "treatment"
  ) |>
  dplyr::mutate(
    treatment = forcats::fct_rev(factor(treatment))
  ) |>
  ggplot2::ggplot(
    ggplot2::aes(
      x = x,
      y = y,
      fill = value,
      color = value
    )
  ) +
  ggplot2::geom_tile() +
  ggplot2::facet_wrap(
    ~ treatment
  ) +
  ggplot2::coord_fixed() +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    legend.position = "none",
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank()
  )

add_fig_no(
  plot = image_plot,
  text = "Example image adjustments for an A sample"
)
```

Using this new data, I was able to maintain good performance of the neural network across all samples using the 200 neuron model even when randomness is introduced into the model images. My neural network has the following specifications from `deepspace`:

| Metric                     | Value          |
|----------------------------|----------------|
| Final Loss                 | 43.574         |
| Estimation Time (CPU Time) | 9.09 Minutes   |
| Network Layer Dimensions   | 1764-200-200-3 |
| Learning Rate Adjustment   | 1e-05          |
| Number of Epochs           | 235            |
| Converged                  | True           |
| Train Accuracy             | 82%            |
| Test Accuracy              | 76%            |
| Augmented Test Accuracy    | 68%            |

# Resources

If you are interested in using `deepspace` in your own work, you can install it directly from GitHub in an R environment. The package comes with two data sets. The first is a sample of the `mnist` digits data set and the other is `asl` which represents the images I captured for my project. To get started, simply install, load, and fit a model!

```{r eval=FALSE}
devtools::install_github("https://github.com/walkerjameschris/deepspace")

library(deepspace)

data <- deepspace:::mnist

network <-
  deepspace::fit_network(
    X = data$X,
    Y = data$Y,
    neurons = 5,
    epoch = 100,
    learn_rate = 0.0001
  )

predict(network)
```

**References:**

-   [3B1B Neural Network Guide](https://www.3blue1brown.com/topics/neural-networks)
-   [IEEE Explore ASL Neural Networks](https://ieeexplore.ieee.org/abstract/document/8650241)
-   [IEEE Explore C++ Neural Networks](https://ieeexplore.ieee.org/abstract/document/4924772)
