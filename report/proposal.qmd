---
title: "Using Neural Networks to Classify American Sign Language (ASL) Signs"
author: "Chris Walker"
format: pdf
editor: visual
---

```{r, echo=FALSE}
github <- "https://github.com/walkerjameschris/deepspace"
```

# Problem Statement and Introduction

Machine learning plays a unique role in improving accessibility in digital communication. This includes services like text-to-speech, language translation, and alt-text generation. Machine learning could also be used to detect American Sign Language (ASL) signs and convert them to text data. **Sign language detection is not currently implemented into the modern ecosystem of video call tools.** Imagine if FaceTime was able to convert signs to text in real time. An ASL user could sign into the camera, convert their message to text, and have it read back as text-to-speech. This could create more natural telecommunication for people with a hearing disability. With this concept in mind, I set out to create a primitive sign language detection model by implementing the learning algorithms from scratch.

The ideal output from this project is a package of C++ and R code combined with a final report that details how the data was collected, how the algorithm was implemented, and most importantly how it performs over a range of test cases. I also want to set reasonable expectations for this class project; while a fully featured model would capture the entirety of ASL, I aim to classify at least several letters of the ASL alphabet as a proof of concept. I expect users to launch their webcam so a live video feed can be delivered to the model for sign classification.

# Data Sources

This model will be exclusively trained on images I capture using a webcam. I will select a few signs to begin with and will sign them in a variety of lighting conditions and background environments. I expect this this to comprise several thousand labelled photos. These images will be saved as directories of `.jpeg` files each representing a given letter in the ASL alphabet.

Like other assignments from this semester, I will vectorize the images into two-dimensional matrices paired with an associated labels matrix which will be one-hot encoded for training in the neural network.

# Methodology

To capture and process these images, I have created a new R package with a handful of helpful functions. This package will also include functions for fitting and evaluating the neural network using a linking library called `Rcpp` which allows me to write very performant C++ functions (i.e., implementing feed forwards, back propagation, and other linear algebra functions) while calling them from a scripting language like R. I will host and document this package on my personal GitHub profile. [You can see the package here.](https://github.com/walkerjameschris/deepspace).

As part of this process I also want to better understand the mathematics behind neural networks, specifically the process of back propagation through the network while training. While there are plenty of well known neural network implementations in R, Python, and MATLAB, I want to come out of this experience with a better working knowledge of how these networks learn. As an example, rather than rely on a pre-built implementation of matrix multiplication, I will program even basic linear algebra operations myself to build up a foundational understanding:

``` cpp
// [[Rcpp::export]]
NumericMatrix mul(NumericMatrix X, NumericMatrix Y) {

  int X_row = X.nrow();
  int X_col = X.ncol();
  int Y_col = Y.ncol();
  
  NumericMatrix result(X_row, Y_col);
  
  for (int i = 0; i < X_row; i++) {
    for (int j = 0; j < Y_col; j++) {
      for (int k = 0; k < X_col; k++) {
        result(i, j) += X(i, k) * Y(k, j);
      }
    }
  }
  
  return result;
}
```

# Evaluation and Final Results

I will train several neural networks using my vectorized images. This will involve several rounds of hyper parameter tuning (i.e, the number of hidden layer neurons) in addition to preprocessing/feature engineering experiments to see which methodology performs the best. I plan to answer the following questions:

-   How does the number of hidden-layer neurons impact performance?

-   Does training on a covariance matrix or the raw images better reduce error?

-   How does the maximum number of epochs and the learning rate alter model fit?

-   Are the first *k* principle components sufficient for model training?

This may also involve testing different activation functions such as the logistic, softmax, and ReLU. I expect to refine my neural network implementation to enhance its robustness and convergence speed.

While several methods exist to measure the performance of classification models with multiple class outcomes, I expect to generally use accuracy (the share of correctly labeled classes over all testing observations) to assess and compare neural networks. I will recommend a pre-trained model in the final report which best distinguishes between signs in a wide variety of lighting environments while also adhering to the parsimony principle. That is, what is the smallest model which provides the best performance across a range of inputs?

One other measure of success for this project is the speed in which the algorithm can feed forward new images to obtain a prediction. Even if the model is reasonably performant at classifying images, it must also be able to generate new predictions quickly. This is likely related to the parsimony principle such that a smaller model can feed forward more quickly than a larger model allowing for quicker classification of signs.

I will also use this as an opportunity to develop quality-of-life functionality for my R package including `predict()` and `plot()` methods which can be used to make predictions and plot the neural network (as shown in the README on the package).

# Hypothesis

I expect that there are diminishing marginal returns to model performance for an increase in the number of hidden layer neurons. Therefore, a network with 2 hidden layers each with 10 neurons may be sufficient. Additionally, I expect that the logistic activation function will be sufficient for model training. I will verify this hypothesis through several rounds of training, hyperparameter tuning, and across several random seeds.

While an increased number of neurons, hidden layers, or training epochs may marginally improve performance, I expect that a fairly simple model will perform adequately for the purposes of image classification. In my final report I will detail the performance of each model training configuration and identify a candidate model. This candidate model can be used for further robustness testing and edge case classification so it can be refined into a final model.
